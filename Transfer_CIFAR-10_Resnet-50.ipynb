{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1 import keras\n",
    "from tensorflow.compat.v1.keras import layers,datasets,utils,models\n",
    "from tensorflow.compat.v1.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.compat.v1.keras.models import Sequential,Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Dropout, Activation, Flatten,Input\n",
    "from tensorflow.compat.v1.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalMaxPooling2D,GlobalAveragePooling2D\n",
    "from tensorflow.compat.v1.keras.applications.resnet50 import ResNet50\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于gpu内存内存不足以存下所有的图片，启动灵活分配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定一下batch大小，类别数，训练轮数，还有就是启用数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载以及预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集实现下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.random.permutation(x_train.shape[0])\n",
    "test_index = np.random.permutation(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3)\n",
      "(5000, 1)\n",
      "(1000, 32, 32, 3)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "little_x_train = (x_train[train_index,:,:,:])[:5000, :,:,:]\n",
    "little_y_train = (y_train[train_index,:])[:5000,:]\n",
    "little_x_test = (x_test[test_index,:,:,:])[:1000,:,:,:]\n",
    "little_y_test = (y_test[test_index,:])[:1000,:]\n",
    "print(little_x_train.shape)\n",
    "print(little_y_train.shape)\n",
    "print(little_x_test.shape)\n",
    "print(little_y_test.shape)\n",
    "x_train = little_x_train\n",
    "y_train = little_y_train\n",
    "x_test = little_x_test\n",
    "y_test = little_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15c74ff5ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据归一化、独热编码化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载预训练的resnet-50模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = ResNet50(include_top = False,pooling = 'max',weights = 'imagenet')\n",
    "Inp = Input((32, 32, 3))\n",
    "#x = GlobalAveragePooling2D()(basemodel(Inp))\n",
    "x = Dense(1024,activation = 'relu')(basemodel(Inp))\n",
    "predictions = Dense(num_classes,activation = 'softmax')(x)\n",
    "x.trainable = True\n",
    "resnet50_fine_tune = Model(inputs=Inp, outputs=predictions)\n",
    "\n",
    "for each in basemodel.layers:\n",
    "    each.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 23,608,202\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet50_fine_tune = keras.models.Sequential()\n",
    "resnet50_fine_tune.add(ResNet50(include_top = False,\n",
    "                                pooling = 'max',\n",
    "                                weights = 'imagenet'))\n",
    "resnet50_fine_tune.add(Dense(num_classes,activation = 'softmax'))\n",
    "resnet50_fine_tune.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 25,696,138\n",
      "Trainable params: 2,108,426\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "resnet50_fine_tune.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics = [\"accuracy\"])\n",
    "resnet50_fine_tune.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开启tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='.\\\\Graph1', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开启训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 41s 132ms/step - loss: 2.1554 - accuracy: 0.3014 - val_loss: 3.4045 - val_accuracy: 0.0870\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.7923 - accuracy: 0.3698 - val_loss: 3.4185 - val_accuracy: 0.1220\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.7485 - accuracy: 0.3840 - val_loss: 4.1530 - val_accuracy: 0.0870\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7270 - accuracy: 0.3962 - val_loss: 3.0574 - val_accuracy: 0.0870\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.6928 - accuracy: 0.4054 - val_loss: 4.1035 - val_accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True, callbacks=[tbCallBack])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    '''\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    '''\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    resnet50_fine_tune.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggmodel = conv_model()\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "vggmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggCallBack = keras.callbacks.TensorBoard(log_dir='.\\\\Graph2', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "312/312 [==============================] - 17s 55ms/step - loss: 2.2863 - accuracy: 0.1069 - val_loss: 2.2197 - val_accuracy: 0.1900\n",
      "Epoch 2/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 2.1507 - accuracy: 0.1756 - val_loss: 2.0239 - val_accuracy: 0.2180\n",
      "Epoch 3/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 2.0333 - accuracy: 0.2113 - val_loss: 1.9077 - val_accuracy: 0.2410\n",
      "Epoch 4/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.9661 - accuracy: 0.2372 - val_loss: 1.9232 - val_accuracy: 0.2590\n",
      "Epoch 5/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.9376 - accuracy: 0.2490 - val_loss: 1.8517 - val_accuracy: 0.3210\n",
      "Epoch 6/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.9071 - accuracy: 0.2695 - val_loss: 1.7925 - val_accuracy: 0.3250\n",
      "Epoch 7/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.8767 - accuracy: 0.2867 - val_loss: 1.8238 - val_accuracy: 0.3380\n",
      "Epoch 8/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.8353 - accuracy: 0.3084 - val_loss: 1.7233 - val_accuracy: 0.3410\n",
      "Epoch 9/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.7961 - accuracy: 0.3170 - val_loss: 1.7448 - val_accuracy: 0.3610\n",
      "Epoch 10/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.7724 - accuracy: 0.3258 - val_loss: 1.6446 - val_accuracy: 0.3830\n",
      "Epoch 11/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.7358 - accuracy: 0.3431 - val_loss: 1.6430 - val_accuracy: 0.3980\n",
      "Epoch 12/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.7016 - accuracy: 0.3593 - val_loss: 1.6431 - val_accuracy: 0.3770\n",
      "Epoch 13/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.6835 - accuracy: 0.3696 - val_loss: 1.6708 - val_accuracy: 0.3760\n",
      "Epoch 14/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.6519 - accuracy: 0.3872 - val_loss: 1.6213 - val_accuracy: 0.3890\n",
      "Epoch 15/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.6336 - accuracy: 0.3874 - val_loss: 1.5447 - val_accuracy: 0.4290\n",
      "Epoch 16/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.6264 - accuracy: 0.3902 - val_loss: 1.5268 - val_accuracy: 0.4370\n",
      "Epoch 17/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.5921 - accuracy: 0.4007 - val_loss: 1.6912 - val_accuracy: 0.4000\n",
      "Epoch 18/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.5906 - accuracy: 0.4011 - val_loss: 1.4773 - val_accuracy: 0.4520\n",
      "Epoch 19/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.5539 - accuracy: 0.4191 - val_loss: 1.4945 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.5301 - accuracy: 0.4380 - val_loss: 1.4595 - val_accuracy: 0.4600\n",
      "Epoch 21/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.4894 - accuracy: 0.4390 - val_loss: 1.4917 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.4754 - accuracy: 0.4498 - val_loss: 1.4602 - val_accuracy: 0.4530\n",
      "Epoch 23/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.4816 - accuracy: 0.4446 - val_loss: 1.4612 - val_accuracy: 0.4590\n",
      "Epoch 24/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.4317 - accuracy: 0.4645 - val_loss: 1.5415 - val_accuracy: 0.4720\n",
      "Epoch 25/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.4361 - accuracy: 0.4597 - val_loss: 1.4648 - val_accuracy: 0.4490\n",
      "Epoch 26/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.4084 - accuracy: 0.4785 - val_loss: 1.4326 - val_accuracy: 0.4770\n",
      "Epoch 27/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.3879 - accuracy: 0.4856 - val_loss: 1.4021 - val_accuracy: 0.4960\n",
      "Epoch 28/30\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 1.3922 - accuracy: 0.4837 - val_loss: 1.3767 - val_accuracy: 0.5080\n",
      "Epoch 29/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.3617 - accuracy: 0.4950 - val_loss: 1.4095 - val_accuracy: 0.4770\n",
      "Epoch 30/30\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 1.3697 - accuracy: 0.4936 - val_loss: 1.5498 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ccaa01f320>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test), callbacks=[vggCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggmodel.load_weights('./model/initmethods_weigths.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = np.argmax(resnet50_fine_tune.predict(x_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print(np.argmax(y_train[:10], axis=1))\n",
    "print(pre[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0988\n"
     ]
    }
   ],
   "source": [
    "sucess = len([each for each in (pre == np.argmax(y_train, axis=1)) if each==True])/len(pre)\n",
    "print(sucess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf3.6]",
   "language": "python",
   "name": "conda-env-tf3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
